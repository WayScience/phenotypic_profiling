{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pathlib\n",
    "import warnings\n",
    "import itertools\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import (\n",
    "    StratifiedKFold,\n",
    "    GridSearchCV,\n",
    ")\n",
    "from sklearn.utils import shuffle, parallel_backend\n",
    "from sklearn.exceptions import ConvergenceWarning\n",
    "from joblib import dump\n",
    "\n",
    "import sys\n",
    "\n",
    "sys.path.append(\"../utils\")\n",
    "from split_utils import get_features_data\n",
    "from train_utils import get_dataset, get_X_y_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "warnings.simplefilter(\"ignore\", category=ConvergenceWarning)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Specify load paths, dataset types, feature types, model types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set numpy seed to make random operations reproduceable\n",
    "np.random.seed(0)\n",
    "\n",
    "# create results directory\n",
    "results_dir = pathlib.Path(\"models/multi_class_models/\")\n",
    "results_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# load training data from indexes and features dataframe\n",
    "data_split_dir = pathlib.Path(f\"../1.split_data/indexes/data_split_indexes.tsv\")\n",
    "labeled_data_dir = pathlib.Path(\"../0.download_data/data/labeled_data.csv.gz\")\n",
    "\n",
    "# specify dataset type, model types, feature, and balance types\n",
    "model_types = [\"final\", \"shuffled_baseline\"]\n",
    "feature_types = [\"CP\", \"DP\", \"CP_and_DP\", \"CP_zernike_only\", \"CP_areashape_only\"]\n",
    "balance_types = [\"balanced\", \"unbalanced\"]\n",
    "dataset_types = [\"ic\", \"no_ic\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Specify parameters to tune for"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameters being tested during grid search: {'C': array([1.e-03, 1.e-02, 1.e-01, 1.e+00, 1.e+01, 1.e+02, 1.e+03]), 'l1_ratio': array([0. , 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1. ])}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "parameters = {\"C\": np.logspace(-3, 3, 7), \"l1_ratio\": np.linspace(0, 1, 11)}\n",
    "print(f\"Parameters being tested during grid search: {parameters}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Find best parameters, train models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training model for: \n",
      "Model Type: final \n",
      "Feature Type: CP \n",
      "Balance Type: balanced \n",
      "Dataset Type: ic\n",
      "X has shape (2432, 157), y has shape (2432,)\n"
     ]
    }
   ],
   "source": [
    "for model_type, feature_type, balance_type, dataset_type in itertools.product(\n",
    "    model_types, feature_types, balance_types, dataset_types\n",
    "):\n",
    "    # print what combination of types we are training model for\n",
    "    print(\n",
    "        f\"Training model for: \\nModel Type: {model_type} \\nFeature Type: {feature_type} \\nBalance Type: {balance_type} \\nDataset Type: {dataset_type}\"\n",
    "    )\n",
    "    \n",
    "    # load training data from indexes and features dataframe\n",
    "    data_split_path = pathlib.Path(f\"../1.split_data/indexes/data_split_indexes__{dataset_type}.tsv\")\n",
    "    features_dataframe_path = pathlib.Path(f\"../0.download_data/data/labeled_data__{dataset_type}.csv.gz\")\n",
    "    \n",
    "    # dataframe with only the labeled data we want (exclude certain phenotypic classes)\n",
    "    features_dataframe = get_features_data(features_dataframe_path)\n",
    "    data_split_indexes = pd.read_csv(data_split_path, sep=\"\\t\", index_col=0)\n",
    "\n",
    "    # get training data from labeled data\n",
    "    training_data = get_dataset(features_dataframe, data_split_indexes, \"train\")\n",
    "    \n",
    "    # get X,y data from training data\n",
    "    X, y = get_X_y_data(\n",
    "                training_data,\n",
    "                feature_type,\n",
    "            )\n",
    "    print(f\"X has shape {X.shape}, y has shape {y.shape}\")\n",
    "    \n",
    "    # shuffle columns of X (features) dataframe independently to create shuffled baseline\n",
    "    if model_type == \"shuffled_baseline\":\n",
    "        for column in X.T:\n",
    "            np.random.shuffle(column)\n",
    "            \n",
    "    # fit grid search cv to X and y data\n",
    "    with parallel_backend(\"multiprocessing\"):\n",
    "        \n",
    "        # create stratified data sets for k-fold cross validation\n",
    "        straified_k_folds = StratifiedKFold(n_splits=10, shuffle=False)\n",
    "        \n",
    "        # create logistic regression model with following parameters\n",
    "        log_reg_model = LogisticRegression(\n",
    "            penalty=\"elasticnet\",\n",
    "            solver=\"saga\",\n",
    "            class_weight= \"balanced\" if balance_type == \"balanced\" else None,\n",
    "            max_iter=100,\n",
    "            n_jobs=-1,\n",
    "            random_state=0\n",
    "        )\n",
    "        \n",
    "        # create grid search with cross validation with hypertuning params\n",
    "        grid_search_cv = GridSearchCV(\n",
    "            log_reg_model,\n",
    "            parameters,\n",
    "            cv=straified_k_folds,\n",
    "            n_jobs=-1,\n",
    "            scoring=\"f1_weighted\",\n",
    "        )\n",
    "        grid_search_cv = grid_search_cv.fit(X, y)\n",
    "\n",
    "    # print info for best estimator\n",
    "    print(f\"Best parameters: {grid_search_cv.best_params_}\")\n",
    "    print(f\"Score of best estimator: {grid_search_cv.best_score_}\\n\")\n",
    "\n",
    "    # save final estimator\n",
    "    save_path = pathlib.Path(f\"{results_dir}/{model_type}__{feature_type}__{balance_type}__{dataset_type}.joblib\")\n",
    "    dump(\n",
    "        grid_search_cv.best_estimator_,\n",
    "        save_path,\n",
    "    )\n",
    "    \n",
    "    print(f\"Saved trained model to {save_path}\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "vscode": {
   "interpreter": {
    "hash": "f9df586d1764dbc68785000a153dad1832127ac564b5e2e4c94e83fc43160b30"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
