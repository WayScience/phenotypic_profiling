{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pathlib\n",
    "import warnings\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn import preprocessing\n",
    "from sklearn.model_selection import (\n",
    "    StratifiedKFold,\n",
    "    KFold,\n",
    "    GridSearchCV,\n",
    ")\n",
    "from sklearn.utils import shuffle, parallel_backend\n",
    "from sklearn.exceptions import ConvergenceWarning\n",
    "from joblib import dump\n",
    "\n",
    "import sys\n",
    "\n",
    "sys.path.append(\"../utils\")\n",
    "from split_utils import get_features_data\n",
    "from train_utils import get_dataset, get_X_y_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set numpy seed to make random operations reproduceable\n",
    "np.random.seed(0)\n",
    "\n",
    "# load training data from indexes and features dataframe\n",
    "data_split_path = pathlib.Path(f\"../1.split_data/indexes/data_split_indexes.tsv\")\n",
    "features_dataframe_path = pathlib.Path(\"../0.download_data/data/labeled_data.csv.gz\")\n",
    "\n",
    "# dataframe with only the labeled data we want (exclude certain phenotypic classes)\n",
    "features_dataframe = get_features_data(features_dataframe_path)\n",
    "data_split_indexes = pd.read_csv(data_split_path, sep=\"\\t\", index_col=0)\n",
    "\n",
    "# get training data from labeled data\n",
    "training_data = get_dataset(features_dataframe, data_split_indexes, \"train\")\n",
    "#training_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameters being tested during grid search: {'C': array([1.e-03, 1.e-02, 1.e-01, 1.e+00, 1.e+01, 1.e+02, 1.e+03]), 'l1_ratio': array([0. , 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1. ])}\n",
      "\n",
      "Training final model on CP features for Large phenotypic class with 67 positive labels...\n",
      "X has shape (2432, 165), y has shape (2432,)\n",
      "Best parameters: {'C': 0.1, 'l1_ratio': 0.0}\n",
      "Score of best estimator: 0.9609192844525735\n",
      "\n",
      "Training shuffled_baseline model on CP features for Large phenotypic class with 67 positive labels...\n",
      "X has shape (2432, 165), y has shape (2432,)\n",
      "Shuffled!\n",
      "Best parameters: {'C': 0.001, 'l1_ratio': 0.0}\n",
      "Score of best estimator: 0.9589204291297457\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# specify model types and feature types\n",
    "model_types = [\"final\", \"shuffled_baseline\"]\n",
    "feature_types = [\"CP\", \"DP\", \"CP_and_DP\"]\n",
    "\n",
    "# create stratified data sets for k-fold cross validation\n",
    "#straified_k_folds = StratifiedKFold(n_splits=10, shuffle=False)\n",
    "k_folds = KFold(n_splits=10, shuffle=False)\n",
    "\n",
    "# create logistic regression model with following parameters\n",
    "log_reg_model = LogisticRegression(\n",
    "    penalty=\"elasticnet\",\n",
    "    solver=\"saga\",\n",
    "    max_iter=10,\n",
    "    n_jobs=-1,\n",
    "    random_state=0,  # CHANGE MAX ITER TO 100\n",
    ")\n",
    "\n",
    "# specify parameters to tune for\n",
    "parameters = {\"C\": np.logspace(-3, 3, 7), \"l1_ratio\": np.linspace(0, 1, 11)}\n",
    "print(f\"Parameters being tested during grid search: {parameters}\\n\")\n",
    "\n",
    "# create grid search with cross validation with hypertuning params\n",
    "grid_search_cv = GridSearchCV(\n",
    "    log_reg_model, parameters, cv=k_folds, n_jobs=-1, scoring=\"f1_weighted\"\n",
    ")\n",
    "\n",
    "# train model on each combination of model type and feature type\n",
    "for model_type in model_types:\n",
    "    for feature_type in feature_types:\n",
    "        for phenotypic_class in training_data[\"Mitocheck_Phenotypic_Class\"].unique():\n",
    "            # create results directory\n",
    "            results_dir = pathlib.Path(\n",
    "                f\"models/single_class_models/{phenotypic_class}_models/\"\n",
    "            )\n",
    "            results_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "            # get number of labels for this specific phenotypic_class\n",
    "            phenotypic_class_counts = (\n",
    "                training_data.loc[\n",
    "                    training_data[\"Mitocheck_Phenotypic_Class\"] == phenotypic_class\n",
    "                ]\n",
    "            ).shape[0]\n",
    "            print(\n",
    "                f\"Training {model_type} model on {feature_type} features for {phenotypic_class} phenotypic class with {phenotypic_class_counts} positive labels...\"\n",
    "            )\n",
    "\n",
    "            # create deep copy of training data so we can make modifications without affecting original training data\n",
    "            class_training_data = training_data.copy(deep=True)\n",
    "            # convert labels that are not phenotypic class to 0 (negative)\n",
    "            class_training_data.loc[\n",
    "                class_training_data[\"Mitocheck_Phenotypic_Class\"] != phenotypic_class,\n",
    "                \"Mitocheck_Phenotypic_Class\",\n",
    "            ] = f\"Not {phenotypic_class}\"\n",
    "            # convert labels that are phenotypic class to 1 (positive)\n",
    "            # class_training_data.loc[\n",
    "            #     class_training_data[\"Mitocheck_Phenotypic_Class\"] == phenotypic_class,\n",
    "            #     \"Mitocheck_Phenotypic_Class\",\n",
    "            # ] = 1\n",
    "\n",
    "            # get X (features) and y (labels) data\n",
    "            X, y = get_X_y_data(class_training_data, feature_type)\n",
    "            print(f\"X has shape {X.shape}, y has shape {y.shape}\")\n",
    "            \n",
    "            lb = preprocessing.LabelBinarizer()\n",
    "            y = lb.fit_transform(y).ravel()\n",
    "            \n",
    "            \n",
    "\n",
    "            # shuffle columns of X (features) dataframe independently to create shuffled baseline\n",
    "            if model_type == \"shuffled_baseline\":\n",
    "                for column in X.T:\n",
    "                    np.random.shuffle(column)\n",
    "                print(\"Shuffled!\")\n",
    "\n",
    "            # fit grid search cv to X and y data\n",
    "            # capture convergence warning from sklearn\n",
    "            # this warning does not affect the model but takes up lots of space in the output\n",
    "            with parallel_backend(\"multiprocessing\"):\n",
    "                with warnings.catch_warnings():\n",
    "                    warnings.filterwarnings(\n",
    "                        \"ignore\", category=ConvergenceWarning, module=\"sklearn\"\n",
    "                    )\n",
    "                    grid_search_cv = grid_search_cv.fit(X, y)\n",
    "\n",
    "            # print info for best estimator\n",
    "            print(f\"Best parameters: {grid_search_cv.best_params_}\")\n",
    "            print(f\"Score of best estimator: {grid_search_cv.best_score_}\\n\")\n",
    "\n",
    "            # save final estimator\n",
    "            # dump(\n",
    "            #     grid_search_cv.best_estimator_,\n",
    "            #     f\"{results_dir}/{model_type}__{feature_type}.joblib\",\n",
    "            # )\n",
    "            break\n",
    "        break\n",
    "    # break\n",
    "    \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.4 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
